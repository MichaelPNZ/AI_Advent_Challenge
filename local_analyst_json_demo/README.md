# Local Analyst JSON Demo

Учебный минимальный пример: берём небольшой `events.json`, загружаем в SQLite и спрашиваем локальную Llama (1B) о данных. Внешних пакетов не нужно: только stdlib.

## Что внутри
- `data/events.json` — 25 событий: шаг воронки, статус, текст ошибки, длительность.
- `demo.py` — готовый скрипт: генерирует SQL через LLM, выполняет его в SQLite, затем просит LLM коротко объяснить результат.

## Запуск
1. Убедитесь, что ваш Llama сервер слушает OpenAI-совместимый `/v1/chat/completions` (например, `llama.cpp --server`).
2. Выполните:
   ```bash
   cd local_analyst_json_demo
   LLM_BASE_URL=http://127.0.0.1:8080 \
   LLM_MODEL=llama3.2-1b-instruct-q4_k_m \
   python demo.py "какая ошибка чаще всего?"
   ```
   Переменные можно не задавать, если значения по умолчанию подходят.

## Режим «спросил — получил» (HTTP-сервис без ручного запуска скрипта)
1. Стартуем сервис один раз:
   ```bash
   cd local_analyst_json_demo
   python service.py
   ```
2. Спрашиваем из любого места:
   ```bash
   curl "http://127.0.0.1:8001/ask?q=какая%20ошибка%20чаще%20всего?"
   ```
   Ответ придёт в JSON: вопрос, SQL, строки результата и финальный текст модели.

Переменные окружения: `ANALYST_HOST`, `ANALYST_PORT`, `ANALYST_DATA` (путь к JSON); `LLM_PROVIDER/LLM_BASE_URL/LLM_MODEL` — как в demo.py. Если модель молчит, сервис всё равно вернёт ответ с детерминированным SQL и кратким итогом.

## Примеры вопросов
- "какая ошибка чаще всего?"
- "на каком шаге больше всего отвалов?"
- "средняя длительность по шагам"
- "сколько уникальных пользователей дошло до paywall?"

## Как это работает (коротко)
1. Скрипт грузит JSON в SQLite в памяти.
2. Первый запрос к LLM просит сгенерировать SQL (строго SELECT). Добавляется `LIMIT`, если его нет.
3. Выполняем SQL, забираем строки.
4. Второй запрос к LLM: коротко объяснить результат на русском.

Ограничения: модель 1B может ошибаться в SQL. Если запрос упал, скорректируйте вопрос или вручную поправьте SQL и снова вызовите шаг 4.
